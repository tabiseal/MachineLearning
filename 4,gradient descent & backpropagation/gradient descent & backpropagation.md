# 勾配降下法と逆伝播

\( y = wx \) 関数の自由度が有限であるため、新しい予測関数が必要です。

\( y = wx + b \) 関数は自由度を増やすことができます。

単一サンプルの場合

サンプル値が \((x_0, y_0)\)

予測値は \( wx_0 + b \)

分散コスト関数から得られる分散は次のようになります。

$$
\begin{align}
e &= [y_0 - (wx_0 + b)]^2 \notag\\
  &= x_0^2 w^2 + (2x_0 b - 2x_0 y_0) w + (y_0^2 + b^2 - 2y_0 b) \notag\\
\end{align}
$$

\( b = 0 \) の場合

$$
\begin{align}
e &= x_0^2 w^2 + (-2x_0 y_0) w + y_0^2 \notag\\
\end{align}
$$

\( b \) が異なる値を取るときのコスト関数への影響：

定数項 \( b \) が導入されることで、関数の自由度が増加し、予測の精度が向上します。これは、モデルがデータのオフセットを適切に調整できるためです。具体的には、\( b \) を調整することで、モデルがデータポイントの全体的な位置をより正確に反映できるようになります。これにより、誤差 \( e \) が最小化され、モデルのパフォーマンスが向上します。

また、\( b \) の値が異なると、コスト関数の形状が変化し、勾配降下法による最適化の軌跡にも影響を与えます。したがって、最適な \( b \) の値を見つけることは、モデルの精度を向上させるために重要です。

異なる \( w \) と \( b \) を選択するたびに、誤差 \( e \) が異なるため、この曲面は \( b \) を代入した後のコスト関数のグラフであり、最小点はここでの \( w \) と \( b \) の値 \((w_{\text{min}}, b_{\text{min}})\) が予測の誤差（コスト）を最小にすることを示します。この最小点の \( w \) と \( b \) の値を得ることができれば、予測関数に戻し、\( y = w_{\text{min}} x + b_{\text{min}} \) の予測が最良です。

每次取不同的\( w \) 和 \( b \) 都会导致误差 \( e \) 不同，这个曲面也就是带入 \( b \) 后得到的代价函数的图像，最低点代表着这里的 \( w \) 和 \( b \) 的取值 \((w_{\text{min}}, b_{\text{min}})\) 会让预测的误差（代价）最小。如果我们能得到这个最低点的 \( w \) 和 \( b \) 的值，放回到预测函数中，\( y = w_{\text{min}} x + b_{\text{min}} \) 预测就是最好的。

分散損失関数：

$$
\begin{align}
e &= (y_0 - (wx_0 + b))^2 \notag\\
  &= b^2 + (2x_0w - 2y_0)b + (x_0^2w^2 + y_0^2 - 2x_0y_0w) \notag\\
\end{align}
$$

\( e \) は \( w \) の一元二次関数曲線であり、 \( b \) に関しても \( e \) と \( b \) の関係も上に開いた放物線です。 \( w \) を異なる値に取ると形成されます。

代価関数の \( w \) と \( b \) の2つの方向でそれぞれ傾きを求め、まず \( w \) に偏微分し、次に \( b \) に偏微分します。

つまり、偏微分 \(\frac{\partial e}{\partial b}\) と \(\frac{\partial e}{\partial w}\) を求めます。これら2つの偏微分をベクトルと見なし、これら2つのベクトルを組み合わせて新しい合成ベクトルを形成します。この合成ベクトルに沿って下降することは、この曲面がその点で最も速く下降する方法です。

この合成ベクトルは数学では**「勾配」**と呼ばれます。

McCulloch-Pitts神経細胞モデルに従って、我々は一次元の一次線形関数を使用して、神経細胞の樹状突起や軸索の振る舞いをモデル化します。これが予測関数モデルであり、我々が統計的に観測されたデータを予測関数に入力して予測を行うプロセスは**「前向き伝播」**と呼ばれます。これは計算が前から後ろに進むためです。データが予測関数を通過して一度前向き伝播すると、予測値が得られます。予測値と統計的に観測された真の値との間には誤差があり、誤差の評価手段として平方誤差/平均平方誤差を選択します。

誤差と予測関数内のパラメータは、さらに関数的な関係を形成します。この関数を**「コスト関数」**と呼びます。

$$
\begin{align}
e &= (y_{\text{observed}} - (wx + b))^2 \notag\\
\end{align}
$$

誤差を分散で評価するため、これは**「分散代価関数」**とも呼ばれます。

この代価関数を使用して予測関数のパラメータを修正するプロセスは、**「逆伝播」**とも呼ばれます。

逆伝播のパラメータ修正方法は、**勾配降下**アルゴリズムを使用し、学習率 \( \alpha \) は経験的に選択されます。

前向き伝播と逆伝播を繰り返して関数の最小点に到達するプロセスは、「トレーニング」または「学習」と呼ばれます。

