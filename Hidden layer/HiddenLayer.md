# 隠れ層
豆豆は単純に大きいか小さいかで毒があるかどうかを判断するのではなく、特定のサイズ範囲内で毒があり、他の範囲では毒がありません。
この場合、活性化関数を加えた予測モデルであっても、活性化関数を加えない予測モデルであっても、予測は不可能です。なぜなら、数学的にはこれらの関数は任意の範囲内で単調性が同じであり、単調増加または単調減少するからです。しかし、新しい豆豆の毒性は大小が不規則に変動します。このような場合には、単調でない関数が必要です。

予測モデルが単調でなくなるようにするためには、神経元をネットワークとして形成する必要があります。
唯一行ったことは、2つの神経元を追加し、**入力をそれぞれの神経元に送信**して計算することです。

その計算結果を第3の神経元に送り、最終的に出力します。

最初の神経元については、まず \( b_{1\_1}, w_{11\_1}, x \) が線形関数によって計算され、 \( z_{1\_1} = w_{11\_1}x + b_{1\_1} \) となります。

次に、活性化関数を通して最終出力を得ます。つまり、 \( a_{1\_1} = \text{sigmoid}(z_{1\_1}) \) となります。そして、勾配降下法を用いることで、最終的な出力は調整され、現在のような形になります。

第二の神経元についても、まず \( w_{12\_1}, x, b_{2\_1} \) を用いて線形関数で計算し、 \( z_{2\_1} = w_{12\_1}x + b_{2\_1} \) を得ます。

次に、活性化関数を通して最終出力を得ます。つまり、\( a_{2\_1} = \text{sigmoid}(z_{2\_1}) \) となります。そして、勾配降下法を用いることで、最終的な出力はこのような形に調整されます。

これら二つの神経元の最終出力を第三の神経元の入力として使用します。

まず、第三の神経元の線形関数を通じて計算し、重みを掛けた後、以下のようになります。

和を求めた後はこのようになります

この時点で、この神経元の線形演算結果はすでに起伏のある非単調な曲線になっていることがわかります。

次に、活性化関数を通して最終出力を得て、勾配降下アルゴリズムを使用して最終的な出力を以下のように調整します。

これが私たちが求めていた結果です。

つまり、入力を2つの部分に分けて調整し、最後の神経元に送ることで、全体の神経ネットワークが単調でないより多様な関数を形成し、より複雑な問題を解決する能力を持つようになります。

もちろん、ここでは各神経元にバイアス項 \( b \) が含まれており、これは線形関数の切片を意味します。一般的にこれは共通の認識とされているため、通常はこれらの \( b \) を省略して表記します。

別の説明として、神経元を追加すると抽象的な次元が増加し、入力をこれら異なる次元に配置します。各次元は、重みを調整し活性化を行うことで、入力に対する異なる解釈を生成します。最終的にこれら抽象的な次元からの出力を結合し、次元を削減して出力を得ます。このようにして、入力データは複数の抽象的な次元で異なる解釈を受けるため、出力により多くの可能性が生まれます。

同様に、環境中の豆豆の毒性がさらに多くの可能性を持つ場合でも、同様の方法を使用できます。入力に対してより多くの抽象的な次元を追加し、これにより入力の解釈が多様化し、より複雑な分類効果を実現します。

中間に新たに追加されたこれらの神経元ノードは「**隠れ層**」と呼ばれます。

入力はこれらの隠れ層を通過し、層ごとに抽象化と理解が行われます。
縦方向に神経元を追加してより深い隠れ層を作成すると、入力はこれらの隠れ層を通して層ごとに抽象化され、微妙な特徴が抽出されるようになります。これにより、神経ネットワークはより賢くなります。


私たちが適切な複雑さの神経ネットワークを構築し、十分なトレーニングを行うと、ネットワーク内の各神経元のパラメータが異なる値に調整されます。これらの機能が単一の神経元が連結されて組み合わされた全体は、非常に複雑な関数を近似することができます。

関数の最終的な形態は、私たちが収集したトレーニングデータによって決定されます。トレーニングデータが豊富であればあるほど、最終的に得られるモデルは新しい問題をよりよく予測できるようになります。この現象は「**一般化**」と呼ばれます。

モデルの一般化能力は、神経ネットワークが追求する核心的な問題です。

隠れ層が3層を超えるネットワークは「**深層神経ネットワーク（DNN）**」と呼ばれます。


