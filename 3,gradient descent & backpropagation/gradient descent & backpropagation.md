# 勾配降下法と逆伝播

$$
\begin{align}
e&=\frac{1}{m}\sum_{i=0}^m(y_{予測}-y_{答え})^2\notag\\
e&=\frac{1}{m}\sum_{i=0}^m(x_i^2*w^2+(-2x_iy_i)*w+y_i^2)\notag
\end{align}
$$

傾きを使用して \( w \) を「移動」させる

傾きの求め方：

曲線 \( e = aw^2 + bw + c \) の上のある地点の傾き

\( x \) 軸上に2点を取る：\( w \) と \( w + \Delta w \)

すると、2点の座標はそれぞれ \((w, aw^2 + bw + c)\)、\((w + \Delta w, a(w + \Delta w)^2 + b(w + \Delta w) + c)\) になります。

$$
\begin{align}
&\frac{(a(w+\Delta w)+b(w+\Delta w)+c)-(aw^2+bw+c)}{w+\Delta w-w}\notag\\
&=\frac{2aw\Delta w+a\Delta w^2+b\Delta w}{\Delta w}\notag\\
&=2aw+a\Delta w +b\notag\\
&\lim_{\Delta w \rightarrow0}2aw+a\Delta w +b\notag\\
&=2aw+b
\end{align}
$$

合成関数の導関数

導関数の法則

$$
\begin{align}
(f(x)g(x))'&=f'(x)g(x)+f(x)g'(x)\notag\\
(f(x)+g(x))'&=f(x)'+g(x)'\notag\\
\end{align}
$$

\( w \) の調整プロセス：

新しい \( w \) ＝ \( w \) － 傾き（正数/負数）

新しい \( w \) ＝ \( w - \alpha \cdot \) 傾き

このように、曲線の異なる箇所の傾きを基に \( w \) を調整する方法は**勾配降下法**と呼ばれます。

\( w \) が最小点付近に収束したら、勾配降下のプロセスを停止し、この時の \( w \) を予測モデルの \( w \) の値として \( y = f(x) = w_{\text{min}} x \) に代入することで予測を完了します。

### 勾配降下法と正規方程との比較

単一サンプルの場合、そのコスト関数は上に開いた放物線となります。各サンプルも同様です。


この合成コスト関数の最小点が全サンプルの全局最適点です。

すべてのサンプルを使用して勾配降下を直接行います。

このプロセスは明確で滑らかな軌跡であり、**「標準勾配降下法」**または**「バッチ勾配降下法」**と呼ばれます。

**利点：**
- 並列計算が可能
- 全局最適点に収束しやすい

**欠点：**
- 計算量が大きい

もし毎回1つのサンプルのみを使用する場合、そのサンプルの最小点は必ずしも全局最適とは限りませんが、これらの単一サンプルコスト関数で勾配降下を繰り返し行うと、全体の傾向は全局最適点に向かって移動します。これは**正規方程**のようにすべてのサンプルを一度に計算するのとは異なります。

このように、毎回1つのサンプルで行う勾配降下は、その収束過程がランダムに揺れ動く軌跡になるため、**「確率的勾配降下法」**（SGD）とも呼ばれます（確率的勾配降下法は「オンライン学習」とも呼ばれます）。

実際には、最小点付近のこの揺れ動く軌跡は、古典的な**「ブラウン運動」**です。

**利点：**
- 毎回パラメータを更新するため、更新プロセスが速い

**欠点：**
- 並列計算ができない
- 全局最適点に収束しにくい

## ミニバッチ

ミニバッチ勾配降下法

毎回サンプルの中から少数（例：100個または200個）を選んで勾配降下を行います。



